# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9kLMgul3VCDXm62R3wy4o3MocjVplS9

## ***DAY*** ***3***
"""

!pip install -U sentence-transformers scikit-learn

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ðŸ§  Load a pre-trained model that understands job language
model = SentenceTransformer('all-MiniLM-L6-v2')

def compute_similarity(resume_text, jd_text):
    # Get embeddings for both
    embeddings = model.encode([resume_text, jd_text])

    # Compare them using cosine similarity
    sim_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

    # Convert to percentage
    return round(sim_score * 100, 2)

resume_text = """
Experienced data scientist with expertise in Python, SQL, machine learning, and AWS.
Worked on cloud-based projects involving NLP and predictive analytics.
"""

jd_text = """
We are hiring a Data Scientist skilled in Python, AWS, and Machine Learning.
Cloud experience and NLP background preferred.
"""

score = compute_similarity(resume_text, jd_text)

print("ðŸ“Š Resume-JD Similarity Score:")
print(f"{score}%")

def compute_similarity(resume_text, jd_text):
    resume_skills = clean_and_tokenize_jd(resume_text)
    jd_skills = clean_and_tokenize_jd(jd_text)
    common_skills = list(set(resume_skills).intersection(set(jd_skills)))

    embeddings = model.encode([resume_text, jd_text])
    sim_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

    return {
        "similarity": round(sim_score * 100, 2),
        "matched_skills": common_skills
    }
