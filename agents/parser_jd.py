# -*- coding: utf-8 -*-
"""parser_jd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11yph-f8rk2EJ7tjFWjniphwlrrVdAHaP

## **DAY** **1**
"""

!pip install nltk

import nltk
nltk.download('punkt')        # For breaking sentences into words
nltk.download('punkt_tab')
nltk.download('stopwords')    # For removing common words like 'and', 'is', 'the'

import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
def clean_and_tokenize_jd(jd_text):
    # üü° Step 1: Make everything lowercase
    text = jd_text.lower()

    # üü° Step 2: Remove punctuation like . , ? (we don‚Äôt need them)
    text = text.translate(str.maketrans('', '', string.punctuation))

    # üü° Step 3: Break into words (called tokenizing)
    words = word_tokenize(text)

    # üü° Step 4: Remove common boring words (like "the", "and", "is")
    stop_words = set(stopwords.words('english'))
    final_words = [word for word in words if word not in stop_words and len(word) > 2]

    return final_words

sample_jd = """
We are looking for a Data Scientist who knows Python, Machine Learning, and Cloud (AWS, GCP).
Must have strong communication and analytical skills.
"""

print("üß† Useful words from JD:")
print(clean_and_tokenize_jd(sample_jd))

"""
## **DAY** **2**

"""

def extract_skills_from_tokens(tokens):
    # üß† This is a pretend list of known skills (you can expand it later)
    known_skills = {
        'python', 'sql', 'aws', 'gcp', 'azure', 'docker', 'kubernetes',
        'java', 'c++', 'excel', 'nlp', 'machine', 'learning',
        'communication', 'teamwork', 'leadership', 'hadoop', 'spark',
        'tensorflow', 'pytorch', 'cloud', 'git', 'ci/cd', 'rest', 'api'
    }

    # Match known skills with what was found in the JD
    jd_skills = [token for token in tokens if token in known_skills]
    return jd_skills

# Sample JD text
sample_jd = """
We are looking for a Data Scientist with Python, SQL, AWS, and strong communication and leadership.
Experience with Docker and Kubernetes is a plus.
"""

# Clean JD
tokens = clean_and_tokenize_jd(sample_jd)

# Extract skills
skills_found = extract_skills_from_tokens(tokens)

print("üõ†Ô∏è Skills Extracted from JD:")
print(skills_found)
